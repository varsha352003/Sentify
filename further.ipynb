{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "750dea2d-5404-4a67-a9dc-1fb38d881e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>Score</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>Summ_tokenize</th>\n",
       "      <th>Text_tokenize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B000EVG8J2</td>\n",
       "      <td>A1L01D2BD3RKVO</td>\n",
       "      <td>5</td>\n",
       "      <td>Crunchy &amp; Good Gluten-Free Sandwich Cookies!</td>\n",
       "      <td>Having tried a couple of other brands of glute...</td>\n",
       "      <td>['Crunchy', '&amp;', 'Good', 'Gluten', '-', 'Free'...</td>\n",
       "      <td>['Having', 'tried', 'a', 'couple', 'of', 'othe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B0000BXJIS</td>\n",
       "      <td>A3U62RE5XZDP0G</td>\n",
       "      <td>5</td>\n",
       "      <td>great kitty treats</td>\n",
       "      <td>My cat loves these treats. If ever I can't fin...</td>\n",
       "      <td>['great', 'kitty', 'treats']</td>\n",
       "      <td>['My', 'cat', 'loves', 'these', 'treats', '.',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B008FHUFAU</td>\n",
       "      <td>AOXC0JQQZGGB6</td>\n",
       "      <td>3</td>\n",
       "      <td>COFFEE TASTE</td>\n",
       "      <td>A little less than I expected.  It tends to ha...</td>\n",
       "      <td>['COFFEE', 'TASTE']</td>\n",
       "      <td>['A', 'little', 'less', 'than', 'I', 'expected...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B006BXV14E</td>\n",
       "      <td>A3PWPNZVMNX3PA</td>\n",
       "      <td>2</td>\n",
       "      <td>So the Mini-Wheats were too big?</td>\n",
       "      <td>First there was Frosted Mini-Wheats, in origin...</td>\n",
       "      <td>['So', 'the', 'Mini', '-', 'Wheats', 'were', '...</td>\n",
       "      <td>['First', 'there', 'was', 'Frosted', 'Mini', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B007I7Z3Z0</td>\n",
       "      <td>A1XNZ7PCE45KK7</td>\n",
       "      <td>5</td>\n",
       "      <td>Great Taste . . .</td>\n",
       "      <td>and I want to congratulate the graphic artist ...</td>\n",
       "      <td>['Great', 'Taste', '.', '.', '.']</td>\n",
       "      <td>['and', 'I', 'want', 'to', 'congratulate', 'th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ProductId          UserId  Score  \\\n",
       "0  B000EVG8J2  A1L01D2BD3RKVO      5   \n",
       "1  B0000BXJIS  A3U62RE5XZDP0G      5   \n",
       "2  B008FHUFAU   AOXC0JQQZGGB6      3   \n",
       "3  B006BXV14E  A3PWPNZVMNX3PA      2   \n",
       "4  B007I7Z3Z0  A1XNZ7PCE45KK7      5   \n",
       "\n",
       "                                        Summary  \\\n",
       "0  Crunchy & Good Gluten-Free Sandwich Cookies!   \n",
       "1                            great kitty treats   \n",
       "2                                  COFFEE TASTE   \n",
       "3              So the Mini-Wheats were too big?   \n",
       "4                             Great Taste . . .   \n",
       "\n",
       "                                                Text  \\\n",
       "0  Having tried a couple of other brands of glute...   \n",
       "1  My cat loves these treats. If ever I can't fin...   \n",
       "2  A little less than I expected.  It tends to ha...   \n",
       "3  First there was Frosted Mini-Wheats, in origin...   \n",
       "4  and I want to congratulate the graphic artist ...   \n",
       "\n",
       "                                       Summ_tokenize  \\\n",
       "0  ['Crunchy', '&', 'Good', 'Gluten', '-', 'Free'...   \n",
       "1                       ['great', 'kitty', 'treats']   \n",
       "2                                ['COFFEE', 'TASTE']   \n",
       "3  ['So', 'the', 'Mini', '-', 'Wheats', 'were', '...   \n",
       "4                  ['Great', 'Taste', '.', '.', '.']   \n",
       "\n",
       "                                       Text_tokenize  \n",
       "0  ['Having', 'tried', 'a', 'couple', 'of', 'othe...  \n",
       "1  ['My', 'cat', 'loves', 'these', 'treats', '.',...  \n",
       "2  ['A', 'little', 'less', 'than', 'I', 'expected...  \n",
       "3  ['First', 'there', 'was', 'Frosted', 'Mini', '...  \n",
       "4  ['and', 'I', 'want', 'to', 'congratulate', 'th...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv(r'tokenized_reviews.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dede3d5-f485-4d62-b4da-ba9e7d39c4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp=spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e3873b3-7e7a-40da-91d8-b433c530660f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "# Remove \"not\" from the stop word list\n",
    "stop_words.discard(\"not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2603d755-9711-4dcb-9271-3dc79d7ad0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "325"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c620d50c-8316-4803-a5bc-4adfb5fdf056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        ['Having', 'tried', 'a', 'couple', 'of', 'othe...\n",
       "1        ['My', 'cat', 'loves', 'these', 'treats', '.',...\n",
       "2        ['A', 'little', 'less', 'than', 'I', 'expected...\n",
       "3        ['First', 'there', 'was', 'Frosted', 'Mini', '...\n",
       "4        ['and', 'I', 'want', 'to', 'congratulate', 'th...\n",
       "                               ...                        \n",
       "49995    ['I', 'love', 'this', 'tea', '!', ' ', 'I', \"'...\n",
       "49996    ['I', 'like', 'a', 'light', 'strength', 'of', ...\n",
       "49997    ['This', 'is', 'the', 'best', 'sugar', 'out', ...\n",
       "49998    ['I', 'love', 'coconut', 'oil', 'for', 'it', \"...\n",
       "49999    ['My', 'cats', 'like', 'this', 'food', 'and', ...\n",
       "Name: Text_tokenize, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Text_tokenize']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa617f22-9705-4e86-a447-de4ac5a40532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1b9779b-b760-4475-954b-cd689f9fe4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_stop_words(tokens):\n",
    "#     filtered_tokens=[token for token in tokens if token.lower() not in stop_words and  token.isalpha()]\n",
    "#     return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e547c582-31a8-449d-ba5b-e0845d6b1dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_stop_words(tokens):\n",
    "#     filtered_tokens=[token for token in tokens if token.lower() not in stop_words and  token.isalpha()]\n",
    "#     return(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c813b038-231e-40fc-b144-26ea2dd1303d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(tokens):\n",
    "    # Handle the case where tokens might be a string representation\n",
    "    if isinstance(tokens, str):\n",
    "        tokens = eval(tokens)\n",
    "    \n",
    "    # Filter out stop words and keep only alphabetic words\n",
    "    filtered_tokens = [token for token in tokens \n",
    "                      if token.lower() not in stop_words \n",
    "                      and token.isalpha() \n",
    "                      and len(token) > 1]  # Added length check to avoid single characters\n",
    "    return filtered_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6cd4ca9-ac5c-4a4e-b59f-e224eec74b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"processed_text\"]=df['Text_tokenize'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8162ef7a-77ca-4c5a-bd20-c059d2c78c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Having, tried, couple, brands, gluten, free, ...\n",
       "1    [cat, loves, treats, find, house, pop, bolts, ...\n",
       "2    [little, expected, tends, muddy, taste, not, e...\n",
       "3    [Frosted, Mini, Wheats, original, size, Froste...\n",
       "4    [want, congratulate, graphic, artist, putting,...\n",
       "Name: processed_text, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"processed_text\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "047d0f35-c574-4c4a-bf11-bf954b32d018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Crunchy, Good, Gluten, Free, Sandwich, Cookies]\n",
       "1                              [great, kitty, treats]\n",
       "2                                     [COFFEE, TASTE]\n",
       "3                                 [Mini, Wheats, big]\n",
       "4                                      [Great, Taste]\n",
       "Name: processed_summary, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"processed_summary\"]=df['Summ_tokenize'].apply(remove_stop_words)\n",
    "df[\"processed_summary\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b894d5c-c34d-4f47-93cc-c201b3f619ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(r'C:\\Users\\Asus\\Contacts\\Desktop\\data sci work\\nlp\\sa+rs\\stop_words_removed.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c435fadf-0d76-44c1-be23-07aa2b6c9327",
   "metadata": {},
   "outputs": [],
   "source": [
    "#next step is lemmatization\n",
    "def lemmatize_tokens(tokens):\n",
    "    if isinstance(tokens, str):\n",
    "        tokens = eval(tokens)\n",
    "    \n",
    "    tokens=[token.lower() for token in tokens]\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    lemmatized_tokens=[token.lemma_ for token in doc]\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1587eba7-de93-4618-882c-3258a7d3de22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [having, try, a, couple, of, other, brand, of,...\n",
       "1    [my, cat, love, these, treat, ., if, ever, I, ...\n",
       "2    [a, little, less, than, I, expect, .,   , it, ...\n",
       "3    [first, there, be, frosted, mini, -, wheat, ,,...\n",
       "4    [and, I, want, to, congratulate, the, graphic,...\n",
       "Name: lemmatized_text, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"lemmatized_text\"]=df['Text_tokenize'].apply(lemmatize_tokens)\n",
    "df[\"lemmatized_text\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31ce1a05-02e8-461b-b9a9-821f868cb767",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(r'C:\\Users\\Asus\\Contacts\\Desktop\\data sci work\\nlp\\sa+rs\\lemmatized_reviews.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21d53b3d-59ed-459c-99d8-bc1a89ef12aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "having: AUX\n",
      "try: VERB\n",
      "a: DET\n",
      "couple: NOUN\n",
      "of: ADP\n",
      "other: ADJ\n",
      "brand: NOUN\n",
      "of: ADP\n",
      "gluten: ADJ\n",
      "-: PUNCT\n",
      "free: ADJ\n",
      "sandwich: NOUN\n",
      "cookie: PROPN\n",
      ",: PUNCT\n",
      "these: PRON\n",
      "be: VERB\n",
      "the: DET\n",
      "good: NOUN\n",
      "of: ADP\n",
      "the: DET\n",
      "bunch: NOUN\n",
      ".: PUNCT\n",
      "   : SPACE\n",
      "they: PRON\n",
      "be: VERB\n",
      "crunchy: ADJ\n",
      "and: CCONJ\n",
      "true: ADJ\n",
      "to: ADP\n",
      "the: DET\n",
      "texture: NOUN\n",
      "of: ADP\n",
      "the: DET\n",
      "other: ADJ\n",
      "\": PUNCT\n",
      "real: ADJ\n",
      "\": PUNCT\n",
      "cookie: NOUN\n",
      "that: PRON\n",
      "be: AUX\n",
      "not: PART\n",
      "gluten: VERB\n",
      "-: PUNCT\n",
      "free: ADJ\n",
      ".: PUNCT\n",
      "   : SPACE\n",
      "some: PRON\n",
      "might: AUX\n",
      "think: VERB\n",
      "that: SCONJ\n",
      "the: DET\n",
      "filling: NOUN\n",
      "make: VERB\n",
      "they: PRON\n",
      "a: DET\n",
      "bit: NOUN\n",
      "too: ADV\n",
      "sweet: ADJ\n",
      ",: PUNCT\n",
      "but: CCONJ\n",
      "for: ADP\n",
      "I: PRON\n",
      "that: PRON\n",
      "just: ADV\n",
      "mean: VERB\n",
      "I: PRON\n",
      "': AUX\n",
      "ve: AUX\n",
      "satisfy: VERB\n",
      "my: PRON\n",
      "sweet: ADJ\n",
      "tooth: NOUN\n",
      "soon: ADV\n",
      "!: PUNCT\n",
      "   : SPACE\n",
      "the: DET\n",
      "chocolate: NOUN\n",
      "version: NOUN\n",
      "from: ADP\n",
      "glutino: PROPN\n",
      "be: AUX\n",
      "just: ADV\n",
      "as: ADV\n",
      "good: ADJ\n",
      "and: CCONJ\n",
      "have: VERB\n",
      "a: DET\n",
      "true: ADJ\n",
      "\": PUNCT\n",
      "chocolatey: ADP\n",
      "\": PUNCT\n",
      "taste: NOUN\n",
      "-: PUNCT\n",
      "something: PRON\n",
      "that: PRON\n",
      "be: VERB\n",
      "not: PART\n",
      "there: ADV\n",
      "with: ADP\n",
      "the: DET\n",
      "other: ADJ\n",
      "gluten: ADJ\n",
      "-: PUNCT\n",
      "free: ADJ\n",
      "brand: NOUN\n",
      "out: ADV\n",
      "there: ADV\n",
      ".: PUNCT\n"
     ]
    }
   ],
   "source": [
    "first_row = df['lemmatized_text'][0]\n",
    "\n",
    "# Convert the list of words into a string\n",
    "first_row_text = ' '.join(first_row)\n",
    "\n",
    "# Process the first row text with spaCy for POS tagging\n",
    "doc = nlp(first_row_text)\n",
    "\n",
    "# Print POS tags for the tokens in the first row\n",
    "for token in doc:\n",
    "    print(f'{token.text}: {token.pos_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c0570c05-1889-48cd-8b97-8c89ea49a351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [having, try, a, couple, of, other, brand, of,...\n",
      "1    [my, cat, love, these, treat, if, ever, I, can...\n",
      "2    [a, little, less, than, I, expect,   , it, ten...\n",
      "3    [first, there, be, frosted, mini, wheat, in, o...\n",
      "4    [and, I, want, to, congratulate, the, graphic,...\n",
      "Name: final_data, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Assuming the DataFrame df has a column called 'lemmatized_text' containing lists of words\n",
    "df[\"final_data\"] = [\n",
    "    [word for word in sentence if not re.match(r'[^\\w\\s]', word)] \n",
    "    for sentence in df[\"lemmatized_text\"]\n",
    "]\n",
    "\n",
    "# Print the first few rows of the cleaned data\n",
    "print(df[\"final_data\"].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c80c870e-2671-46e2-b5bd-d1f094fb58ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'C:\\Users\\Asus\\Contacts\\Desktop\\data sci work\\nlp\\sa+rs\\lemmatized_reviews2.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0897cc56-2f26-492d-83a1-082d2ea981f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>Score</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>Summ_tokenize</th>\n",
       "      <th>Text_tokenize</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>processed_summary</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>cleaned_data</th>\n",
       "      <th>final_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B000EVG8J2</td>\n",
       "      <td>A1L01D2BD3RKVO</td>\n",
       "      <td>5</td>\n",
       "      <td>Crunchy &amp; Good Gluten-Free Sandwich Cookies!</td>\n",
       "      <td>Having tried a couple of other brands of glute...</td>\n",
       "      <td>['Crunchy', '&amp;', 'Good', 'Gluten', '-', 'Free'...</td>\n",
       "      <td>['Having', 'tried', 'a', 'couple', 'of', 'othe...</td>\n",
       "      <td>[Having, tried, couple, brands, gluten, free, ...</td>\n",
       "      <td>[Crunchy, Good, Gluten, Free, Sandwich, Cookies]</td>\n",
       "      <td>[having, try, a, couple, of, other, brand, of,...</td>\n",
       "      <td>[having, try, a, couple, of, other, brand, of,...</td>\n",
       "      <td>[having, try, a, couple, of, other, brand, of,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B0000BXJIS</td>\n",
       "      <td>A3U62RE5XZDP0G</td>\n",
       "      <td>5</td>\n",
       "      <td>great kitty treats</td>\n",
       "      <td>My cat loves these treats. If ever I can't fin...</td>\n",
       "      <td>['great', 'kitty', 'treats']</td>\n",
       "      <td>['My', 'cat', 'loves', 'these', 'treats', '.',...</td>\n",
       "      <td>[cat, loves, treats, find, house, pop, bolts, ...</td>\n",
       "      <td>[great, kitty, treats]</td>\n",
       "      <td>[my, cat, love, these, treat, ., if, ever, I, ...</td>\n",
       "      <td>[my, cat, love, these, treat, if, ever, I, can...</td>\n",
       "      <td>[my, cat, love, these, treat, if, ever, I, can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B008FHUFAU</td>\n",
       "      <td>AOXC0JQQZGGB6</td>\n",
       "      <td>3</td>\n",
       "      <td>COFFEE TASTE</td>\n",
       "      <td>A little less than I expected.  It tends to ha...</td>\n",
       "      <td>['COFFEE', 'TASTE']</td>\n",
       "      <td>['A', 'little', 'less', 'than', 'I', 'expected...</td>\n",
       "      <td>[little, expected, tends, muddy, taste, not, e...</td>\n",
       "      <td>[COFFEE, TASTE]</td>\n",
       "      <td>[a, little, less, than, I, expect, .,   , it, ...</td>\n",
       "      <td>[a, little, less, than, I, expect,   , it, ten...</td>\n",
       "      <td>[a, little, less, than, I, expect,   , it, ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B006BXV14E</td>\n",
       "      <td>A3PWPNZVMNX3PA</td>\n",
       "      <td>2</td>\n",
       "      <td>So the Mini-Wheats were too big?</td>\n",
       "      <td>First there was Frosted Mini-Wheats, in origin...</td>\n",
       "      <td>['So', 'the', 'Mini', '-', 'Wheats', 'were', '...</td>\n",
       "      <td>['First', 'there', 'was', 'Frosted', 'Mini', '...</td>\n",
       "      <td>[Frosted, Mini, Wheats, original, size, Froste...</td>\n",
       "      <td>[Mini, Wheats, big]</td>\n",
       "      <td>[first, there, be, frosted, mini, -, wheat, ,,...</td>\n",
       "      <td>[first, there, be, frosted, mini, wheat, in, o...</td>\n",
       "      <td>[first, there, be, frosted, mini, wheat, in, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B007I7Z3Z0</td>\n",
       "      <td>A1XNZ7PCE45KK7</td>\n",
       "      <td>5</td>\n",
       "      <td>Great Taste . . .</td>\n",
       "      <td>and I want to congratulate the graphic artist ...</td>\n",
       "      <td>['Great', 'Taste', '.', '.', '.']</td>\n",
       "      <td>['and', 'I', 'want', 'to', 'congratulate', 'th...</td>\n",
       "      <td>[want, congratulate, graphic, artist, putting,...</td>\n",
       "      <td>[Great, Taste]</td>\n",
       "      <td>[and, I, want, to, congratulate, the, graphic,...</td>\n",
       "      <td>[and, I, want, to, congratulate, the, graphic,...</td>\n",
       "      <td>[and, I, want, to, congratulate, the, graphic,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ProductId          UserId  Score  \\\n",
       "0  B000EVG8J2  A1L01D2BD3RKVO      5   \n",
       "1  B0000BXJIS  A3U62RE5XZDP0G      5   \n",
       "2  B008FHUFAU   AOXC0JQQZGGB6      3   \n",
       "3  B006BXV14E  A3PWPNZVMNX3PA      2   \n",
       "4  B007I7Z3Z0  A1XNZ7PCE45KK7      5   \n",
       "\n",
       "                                        Summary  \\\n",
       "0  Crunchy & Good Gluten-Free Sandwich Cookies!   \n",
       "1                            great kitty treats   \n",
       "2                                  COFFEE TASTE   \n",
       "3              So the Mini-Wheats were too big?   \n",
       "4                             Great Taste . . .   \n",
       "\n",
       "                                                Text  \\\n",
       "0  Having tried a couple of other brands of glute...   \n",
       "1  My cat loves these treats. If ever I can't fin...   \n",
       "2  A little less than I expected.  It tends to ha...   \n",
       "3  First there was Frosted Mini-Wheats, in origin...   \n",
       "4  and I want to congratulate the graphic artist ...   \n",
       "\n",
       "                                       Summ_tokenize  \\\n",
       "0  ['Crunchy', '&', 'Good', 'Gluten', '-', 'Free'...   \n",
       "1                       ['great', 'kitty', 'treats']   \n",
       "2                                ['COFFEE', 'TASTE']   \n",
       "3  ['So', 'the', 'Mini', '-', 'Wheats', 'were', '...   \n",
       "4                  ['Great', 'Taste', '.', '.', '.']   \n",
       "\n",
       "                                       Text_tokenize  \\\n",
       "0  ['Having', 'tried', 'a', 'couple', 'of', 'othe...   \n",
       "1  ['My', 'cat', 'loves', 'these', 'treats', '.',...   \n",
       "2  ['A', 'little', 'less', 'than', 'I', 'expected...   \n",
       "3  ['First', 'there', 'was', 'Frosted', 'Mini', '...   \n",
       "4  ['and', 'I', 'want', 'to', 'congratulate', 'th...   \n",
       "\n",
       "                                      processed_text  \\\n",
       "0  [Having, tried, couple, brands, gluten, free, ...   \n",
       "1  [cat, loves, treats, find, house, pop, bolts, ...   \n",
       "2  [little, expected, tends, muddy, taste, not, e...   \n",
       "3  [Frosted, Mini, Wheats, original, size, Froste...   \n",
       "4  [want, congratulate, graphic, artist, putting,...   \n",
       "\n",
       "                                  processed_summary  \\\n",
       "0  [Crunchy, Good, Gluten, Free, Sandwich, Cookies]   \n",
       "1                            [great, kitty, treats]   \n",
       "2                                   [COFFEE, TASTE]   \n",
       "3                               [Mini, Wheats, big]   \n",
       "4                                    [Great, Taste]   \n",
       "\n",
       "                                     lemmatized_text  \\\n",
       "0  [having, try, a, couple, of, other, brand, of,...   \n",
       "1  [my, cat, love, these, treat, ., if, ever, I, ...   \n",
       "2  [a, little, less, than, I, expect, .,   , it, ...   \n",
       "3  [first, there, be, frosted, mini, -, wheat, ,,...   \n",
       "4  [and, I, want, to, congratulate, the, graphic,...   \n",
       "\n",
       "                                        cleaned_data  \\\n",
       "0  [having, try, a, couple, of, other, brand, of,...   \n",
       "1  [my, cat, love, these, treat, if, ever, I, can...   \n",
       "2  [a, little, less, than, I, expect,   , it, ten...   \n",
       "3  [first, there, be, frosted, mini, wheat, in, o...   \n",
       "4  [and, I, want, to, congratulate, the, graphic,...   \n",
       "\n",
       "                                          final_data  \n",
       "0  [having, try, a, couple, of, other, brand, of,...  \n",
       "1  [my, cat, love, these, treat, if, ever, I, can...  \n",
       "2  [a, little, less, than, I, expect,   , it, ten...  \n",
       "3  [first, there, be, frosted, mini, wheat, in, o...  \n",
       "4  [and, I, want, to, congratulate, the, graphic,...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44383c34-2928-4c28-959c-91641793ee29",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_data\u001b[39m\u001b[38;5;124m'\u001b[39m],axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.drop(['cleaned_data'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45e52200-6f87-4240-811c-dec0886f4581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>Score</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>Summ_tokenize</th>\n",
       "      <th>Text_tokenize</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>processed_summary</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>cleaned_data</th>\n",
       "      <th>final_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B000EVG8J2</td>\n",
       "      <td>A1L01D2BD3RKVO</td>\n",
       "      <td>5</td>\n",
       "      <td>Crunchy &amp; Good Gluten-Free Sandwich Cookies!</td>\n",
       "      <td>Having tried a couple of other brands of glute...</td>\n",
       "      <td>['Crunchy', '&amp;', 'Good', 'Gluten', '-', 'Free'...</td>\n",
       "      <td>['Having', 'tried', 'a', 'couple', 'of', 'othe...</td>\n",
       "      <td>['Having', 'tried', 'couple', 'brands', 'glute...</td>\n",
       "      <td>['Crunchy', 'Good', 'Gluten', 'Free', 'Sandwic...</td>\n",
       "      <td>['having', 'try', 'a', 'couple', 'of', 'other'...</td>\n",
       "      <td>['having', 'try', 'a', 'couple', 'of', 'other'...</td>\n",
       "      <td>['having', 'try', 'a', 'couple', 'of', 'other'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B0000BXJIS</td>\n",
       "      <td>A3U62RE5XZDP0G</td>\n",
       "      <td>5</td>\n",
       "      <td>great kitty treats</td>\n",
       "      <td>My cat loves these treats. If ever I can't fin...</td>\n",
       "      <td>['great', 'kitty', 'treats']</td>\n",
       "      <td>['My', 'cat', 'loves', 'these', 'treats', '.',...</td>\n",
       "      <td>['cat', 'loves', 'treats', 'find', 'house', 'p...</td>\n",
       "      <td>['great', 'kitty', 'treats']</td>\n",
       "      <td>['my', 'cat', 'love', 'these', 'treat', '.', '...</td>\n",
       "      <td>['my', 'cat', 'love', 'these', 'treat', 'if', ...</td>\n",
       "      <td>['my', 'cat', 'love', 'these', 'treat', 'if', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B008FHUFAU</td>\n",
       "      <td>AOXC0JQQZGGB6</td>\n",
       "      <td>3</td>\n",
       "      <td>COFFEE TASTE</td>\n",
       "      <td>A little less than I expected.  It tends to ha...</td>\n",
       "      <td>['COFFEE', 'TASTE']</td>\n",
       "      <td>['A', 'little', 'less', 'than', 'I', 'expected...</td>\n",
       "      <td>['little', 'expected', 'tends', 'muddy', 'tast...</td>\n",
       "      <td>['COFFEE', 'TASTE']</td>\n",
       "      <td>['a', 'little', 'less', 'than', 'I', 'expect',...</td>\n",
       "      <td>['a', 'little', 'less', 'than', 'I', 'expect',...</td>\n",
       "      <td>['a', 'little', 'less', 'than', 'I', 'expect',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B006BXV14E</td>\n",
       "      <td>A3PWPNZVMNX3PA</td>\n",
       "      <td>2</td>\n",
       "      <td>So the Mini-Wheats were too big?</td>\n",
       "      <td>First there was Frosted Mini-Wheats, in origin...</td>\n",
       "      <td>['So', 'the', 'Mini', '-', 'Wheats', 'were', '...</td>\n",
       "      <td>['First', 'there', 'was', 'Frosted', 'Mini', '...</td>\n",
       "      <td>['Frosted', 'Mini', 'Wheats', 'original', 'siz...</td>\n",
       "      <td>['Mini', 'Wheats', 'big']</td>\n",
       "      <td>['first', 'there', 'be', 'frosted', 'mini', '-...</td>\n",
       "      <td>['first', 'there', 'be', 'frosted', 'mini', 'w...</td>\n",
       "      <td>['first', 'there', 'be', 'frosted', 'mini', 'w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B007I7Z3Z0</td>\n",
       "      <td>A1XNZ7PCE45KK7</td>\n",
       "      <td>5</td>\n",
       "      <td>Great Taste . . .</td>\n",
       "      <td>and I want to congratulate the graphic artist ...</td>\n",
       "      <td>['Great', 'Taste', '.', '.', '.']</td>\n",
       "      <td>['and', 'I', 'want', 'to', 'congratulate', 'th...</td>\n",
       "      <td>['want', 'congratulate', 'graphic', 'artist', ...</td>\n",
       "      <td>['Great', 'Taste']</td>\n",
       "      <td>['and', 'I', 'want', 'to', 'congratulate', 'th...</td>\n",
       "      <td>['and', 'I', 'want', 'to', 'congratulate', 'th...</td>\n",
       "      <td>['and', 'I', 'want', 'to', 'congratulate', 'th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ProductId          UserId  Score  \\\n",
       "0  B000EVG8J2  A1L01D2BD3RKVO      5   \n",
       "1  B0000BXJIS  A3U62RE5XZDP0G      5   \n",
       "2  B008FHUFAU   AOXC0JQQZGGB6      3   \n",
       "3  B006BXV14E  A3PWPNZVMNX3PA      2   \n",
       "4  B007I7Z3Z0  A1XNZ7PCE45KK7      5   \n",
       "\n",
       "                                        Summary  \\\n",
       "0  Crunchy & Good Gluten-Free Sandwich Cookies!   \n",
       "1                            great kitty treats   \n",
       "2                                  COFFEE TASTE   \n",
       "3              So the Mini-Wheats were too big?   \n",
       "4                             Great Taste . . .   \n",
       "\n",
       "                                                Text  \\\n",
       "0  Having tried a couple of other brands of glute...   \n",
       "1  My cat loves these treats. If ever I can't fin...   \n",
       "2  A little less than I expected.  It tends to ha...   \n",
       "3  First there was Frosted Mini-Wheats, in origin...   \n",
       "4  and I want to congratulate the graphic artist ...   \n",
       "\n",
       "                                       Summ_tokenize  \\\n",
       "0  ['Crunchy', '&', 'Good', 'Gluten', '-', 'Free'...   \n",
       "1                       ['great', 'kitty', 'treats']   \n",
       "2                                ['COFFEE', 'TASTE']   \n",
       "3  ['So', 'the', 'Mini', '-', 'Wheats', 'were', '...   \n",
       "4                  ['Great', 'Taste', '.', '.', '.']   \n",
       "\n",
       "                                       Text_tokenize  \\\n",
       "0  ['Having', 'tried', 'a', 'couple', 'of', 'othe...   \n",
       "1  ['My', 'cat', 'loves', 'these', 'treats', '.',...   \n",
       "2  ['A', 'little', 'less', 'than', 'I', 'expected...   \n",
       "3  ['First', 'there', 'was', 'Frosted', 'Mini', '...   \n",
       "4  ['and', 'I', 'want', 'to', 'congratulate', 'th...   \n",
       "\n",
       "                                      processed_text  \\\n",
       "0  ['Having', 'tried', 'couple', 'brands', 'glute...   \n",
       "1  ['cat', 'loves', 'treats', 'find', 'house', 'p...   \n",
       "2  ['little', 'expected', 'tends', 'muddy', 'tast...   \n",
       "3  ['Frosted', 'Mini', 'Wheats', 'original', 'siz...   \n",
       "4  ['want', 'congratulate', 'graphic', 'artist', ...   \n",
       "\n",
       "                                   processed_summary  \\\n",
       "0  ['Crunchy', 'Good', 'Gluten', 'Free', 'Sandwic...   \n",
       "1                       ['great', 'kitty', 'treats']   \n",
       "2                                ['COFFEE', 'TASTE']   \n",
       "3                          ['Mini', 'Wheats', 'big']   \n",
       "4                                 ['Great', 'Taste']   \n",
       "\n",
       "                                     lemmatized_text  \\\n",
       "0  ['having', 'try', 'a', 'couple', 'of', 'other'...   \n",
       "1  ['my', 'cat', 'love', 'these', 'treat', '.', '...   \n",
       "2  ['a', 'little', 'less', 'than', 'I', 'expect',...   \n",
       "3  ['first', 'there', 'be', 'frosted', 'mini', '-...   \n",
       "4  ['and', 'I', 'want', 'to', 'congratulate', 'th...   \n",
       "\n",
       "                                        cleaned_data  \\\n",
       "0  ['having', 'try', 'a', 'couple', 'of', 'other'...   \n",
       "1  ['my', 'cat', 'love', 'these', 'treat', 'if', ...   \n",
       "2  ['a', 'little', 'less', 'than', 'I', 'expect',...   \n",
       "3  ['first', 'there', 'be', 'frosted', 'mini', 'w...   \n",
       "4  ['and', 'I', 'want', 'to', 'congratulate', 'th...   \n",
       "\n",
       "                                          final_data  \n",
       "0  ['having', 'try', 'a', 'couple', 'of', 'other'...  \n",
       "1  ['my', 'cat', 'love', 'these', 'treat', 'if', ...  \n",
       "2  ['a', 'little', 'less', 'than', 'I', 'expect',...  \n",
       "3  ['first', 'there', 'be', 'frosted', 'mini', 'w...  \n",
       "4  ['and', 'I', 'want', 'to', 'congratulate', 'th...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(r'lemmatized_reviews2.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3487f330-faf6-47ed-8b5c-b6cf54a52cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
